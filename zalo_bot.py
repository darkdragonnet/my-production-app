from flask import Flask, request, jsonify
import os
import logging
import requests
import threading

# Import Google GenAI SDK
from google import genai

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("zalo_bot_core")
app = Flask(__name__)

FLASK_API_URL = os.getenv("FLASK_API_URL", "http://zalo-flask-api:5001")

def send_zalo_message(chat_id, text):
    """Gá»i ngÆ°á»£c láº¡i Flask API cá»­a trÆ°á»›c Ä‘á»ƒ tráº£ lá»i Zalo"""
    try:
        payload = {"chat_id": chat_id, "type": "text", "text": text}
        requests.post(f"{FLASK_API_URL}/send-message", json=payload, timeout=15)
    except Exception as e:
        logger.error(f"âŒ Lá»—i nhá» Flask API gá»­i tin: {e}")

# ==========================================
# ðŸ§  1. MAGISTERIUM AI (Lá»‡nh: !ask)
# ==========================================
def call_magisterium_direct(query_text):
    api_key = os.getenv("MAGISTERIUM_API_KEY")
    api_url = os.getenv("MAGISTERIUM_API_URL", "https://www.magisterium.com/api/v1/chat/completions")
    if not api_key:
        return {"success": False, "error_message": "âŒ Thiáº¿u MAGISTERIUM_API_KEY."}

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": "magisterium-1", "messages": [{"role": "user", "content": query_text}], "stream": False}

    try:
        res = requests.post(api_url, headers=headers, json=payload, timeout=60)
        if res.status_code != 200:
            return {"success": False, "error_message": f"âŒ Lá»—i Magisterium: {res.status_code}"}
        data = res.json()
        return {"success": True, "answer": data["choices"][0]["message"]["content"], "citations": data.get("citations", [])}
    except Exception as e:
        return {"success": False, "error_message": f"âŒ Lá»—i máº¡ng: {e}"}

def process_magisterium(sender_id, query):
    send_zalo_message(sender_id, "â³ Magisterium Ä‘ang tra cá»©u tÃ i liá»‡u...")
    res = call_magisterium_direct(query)
    if not res["success"]:
        send_zalo_message(sender_id, res["error_message"])
    else:
        answer_clean = res["answer"]
        if len(answer_clean) > 4000: answer_clean = answer_clean[:3900] + "\n\n[...]"
        
        cits_text = ""
        if res["citations"]:
            cits_text = "ðŸ“š **THAM KHáº¢O:**\n" + "\n".join([f"[{i+1}] {c.get('document_title', '')}" for i, c in enumerate(res["citations"])])
            
        send_zalo_message(sender_id, answer_clean)
        if cits_text: send_zalo_message(sender_id, cits_text)

# ==========================================
# ðŸŒŸ 2. GOOGLE GEMINI (Lá»‡nh: !gemini)
# ==========================================
def process_gemini_ai(sender_id, query):
    try:
        send_zalo_message(sender_id, "â³ Gemini Ä‘ang suy nghÄ©...")
        client = genai.Client() 
        response = client.models.generate_content(
            model="gemini-3-flash", # ÄÃ£ fix model cÃ³ há»— trá»£ Free Quota
            contents=query
        )
        answer = response.text
        if len(answer) > 4000: answer = answer[:3900] + "\n\n[...]"
        send_zalo_message(sender_id, answer)
    except Exception as e:
        logger.error(f"âŒ Lá»—i Gemini AI: {e}")
        send_zalo_message(sender_id, "âŒ Lá»—i káº¿t ná»‘i Gemini.")


# ==========================================
# âš¡ 3. AUTO-REPLY GROQ / NVIDIA (Chat tá»± do)
# ==========================================
def call_openai_compatible_api(api_url, api_key, model_name, query, extra_headers=None):
    """HÃ m lÃµi dÃ¹ng chung Ä‘á»ƒ gá»i API chuáº©n OpenAI (Groq, NVIDIA, OpenRouter)"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    if extra_headers:
        headers.update(extra_headers)
        
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": query}],
        "stream": False,
        "max_tokens": 1500
    }
    
    res = requests.post(api_url, headers=headers, json=payload, timeout=45)
    res.raise_for_status()
    return res.json()["choices"][0]["message"]["content"]


def process_smart_reply(sender_id, query):
    """Worker Thread: Xá»­ lÃ½ chat tá»± do cÃ³ Fallback"""
    send_zalo_message(sender_id, "â³ Bot Ä‘ang suy nghÄ©...")
    answer = ""
    
    groq_key = os.getenv("GROQ_API_KEY")
    nv_key = os.getenv("NVIDIA_API_KEY")

    # --- BÆ¯á»šC 1: THá»¬ GROQ TRÆ¯á»šC ---
    if groq_key:
        try:
            logger.info("âš¡ Äang gá»i Groq (Llama 3.3 Versatile)...")
            answer = call_openai_compatible_api(
                api_url="https://api.groq.com/openai/v1/chat/completions",
                api_key=groq_key,
                model_name="llama-3.3-70b-versatile",
                query=query
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Groq gá»i API lá»—i: {e}. Äang chuyá»ƒn hÆ°á»›ng...")
    else:
        logger.warning("âš ï¸ Bá»Ž QUA GROQ: KhÃ´ng tÃ¬m tháº¥y GROQ_API_KEY")

    # --- BÆ¯á»šC 2: Dá»° PHÃ’NG NVIDIA API ---
    if not answer:
        if nv_key:
            try:
                logger.info("ðŸ”„ Äang Ä‘á»‹nh tuyáº¿n vÃ o NVIDIA API (Llama 3.3 70B)...")
                answer = call_openai_compatible_api(
                    api_url="https://integrate.api.nvidia.com/v1/chat/completions",
                    api_key=nv_key,
                    model_name="meta/llama-3.3-70b-instruct",
                    query=query
                )
            except Exception as e:
                logger.error(f"âŒ NVIDIA API lá»—i: {e}")
        else:
            logger.warning("âš ï¸ Bá»Ž QUA NVIDIA: KhÃ´ng tÃ¬m tháº¥y NVIDIA_API_KEY")

    # --- BÆ¯á»šC 3: Xá»¬ LÃ Káº¾T QUáº¢ ---
    if answer:
        if len(answer) > 4000: answer = answer[:3900] + "\n\n[... Chiá»u dÃ i vÆ°á»£t quÃ¡ giá»›i háº¡n ...]"
        send_zalo_message(sender_id, answer)
        logger.info(f"âœ… ÄÃ£ Auto-Reply thÃ nh cÃ´ng cho user {sender_id}")
    else:
        send_zalo_message(sender_id, "âŒ Bot hiá»‡n Ä‘ang quÃ¡ táº£i hoáº·c lá»—i cáº¥u hÃ¬nh API.")
        logger.error("âŒ Thread Auto-reply káº¿t thÃºc: KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i.")


# ==========================================
# ðŸŽ¯ 4. Gá»ŒI ÄÃCH DANH MODEL QUA NVIDIA API
# ==========================================
def process_nvidia_ai(sender_id, query):
    """Worker Thread: Xá»­ lÃ½ lá»‡nh gá»i !llama qua mÃ¡y chá»§ NVIDIA"""
    send_zalo_message(sender_id, "â³ NVIDIA Llama 3.3 Ä‘ang phÃ¢n tÃ­ch yÃªu cáº§u...")
    
    try:
        nv_key = os.getenv("NVIDIA_API_KEY")
        if not nv_key:
            send_zalo_message(sender_id, f"âŒ Há»‡ thá»‘ng thiáº¿u NVIDIA_API_KEY trong .env.")
            return

        # Gá»i qua hÃ m lÃµi cÃ³ sáºµn, chá»‰ Ä‘á»•i URL vÃ  cáº¥u hÃ¬nh
        answer = call_openai_compatible_api(
            api_url="https://integrate.api.nvidia.com/v1/chat/completions",
            api_key=nv_key,
            model_name="meta/llama-3.3-70b-instruct",
            query=query
        )
        
        if answer:
            if len(answer) > 4000: 
                answer = answer[:3900] + "\n\n[... Chiá»u dÃ i vÆ°á»£t quÃ¡ giá»›i háº¡n ...]"
            send_zalo_message(sender_id, answer)
            logger.info(f"âœ… ÄÃ£ tráº£ lá»i báº±ng NVIDIA Llama thÃ nh cÃ´ng.")
            
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi gá»i NVIDIA API: {e}", exc_info=True)
        send_zalo_message(sender_id, f"âŒ MÃ¡y chá»§ NVIDIA hiá»‡n Ä‘ang báº­n hoáº·c gáº·p sá»± cá»‘.")

# ==========================================
# ðŸšª Bá»˜ Äá»ŠNH TUYáº¾N WEBHOOK (ROUTER)
# ==========================================
@app.route('/webhook/zalo', methods=['POST'])
def receive_webhook():
    data = request.get_json(silent=True) or {}
    payload = data.get("result", data) or {}
    message_obj = payload.get("message", {})
    sender_id = message_obj.get("from", {}).get("id", "")
    message_text = message_obj.get("text", "")

    if sender_id and message_text:
        
        # LUá»’NG 1: Lá»‡nh !ask -> Magisterium
        if message_text.startswith("!ask "):
            query = message_text[5:].strip()
            if query: threading.Thread(target=process_magisterium, args=(sender_id, query), daemon=True).start()
            
        # LUá»’NG 2: Lá»‡nh !gemini -> Google Gemini
        elif message_text.startswith("!gemini "):
            query = message_text[8:].strip()
            if query: threading.Thread(target=process_gemini_ai, args=(sender_id, query), daemon=True).start()
            
        # LUá»’NG 3: Lá»‡nh !llama -> NVIDIA Llama 3.3 70B
        elif message_text.startswith("!llama "):
            query = message_text[7:].strip()
            if query:
                threading.Thread(target=process_nvidia_ai, args=(sender_id, query), daemon=True).start()
                
        # LUá»’NG 4: Chat tá»± nhiÃªn -> Tá»± Ä‘á»™ng dÃ¹ng Groq (allam-2-7b), lá»—i thÃ¬ qua NVIDIA
        else:
            threading.Thread(target=process_smart_reply, args=(sender_id, message_text), daemon=True).start()

    return jsonify({"status": "ok"}), 200

@app.route('/', methods=['GET'])
def health(): 
    return "OK", 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5002)
